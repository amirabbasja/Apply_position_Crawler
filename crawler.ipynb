{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting results for page 1\n",
      "getting results for page 2\n",
      "getting results for page 3\n",
      "getting results for page 4\n",
      "getting results for page 5\n",
      "getting results for page 6\n",
      "getting results for page 7\n",
      "getting results for page 8\n",
      "getting results for page 9\n",
      "getting results for page 10\n",
      "getting results for page 11\n",
      "getting results for page 12\n",
      "getting results for page 13\n",
      "getting results for page 14\n",
      "getting results for page 15\n",
      "getting results for page 16\n",
      "getting results for page 17\n",
      "getting results for page 18\n",
      "getting results for page 19\n",
      "getting results for page 20\n",
      "getting results for page 21\n",
      "getting results for page 22\n",
      "getting results for page 23\n",
      "getting results for page 24\n",
      "getting results for page 25\n",
      "getting results for page 26\n",
      "getting results for page 27\n",
      "getting results for page 28\n",
      "getting results for page 29\n"
     ]
    }
   ],
   "source": [
    "# Crawler for www.findaphd.com\n",
    "\n",
    "df = pd.DataFrame(columns=[\"title\",\"country\",\"university\",\"deadline\",\"lastUpdate\",\"link\"])\n",
    "\n",
    "pageNo = 1\n",
    "while True:\n",
    "    print(f\"getting results for page {pageNo}\")\n",
    "\n",
    "    response = requests.get(f\"https://www.findaphd.com/phds/engineering/?10M7o0&Show=M&PG={pageNo}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    elements = soup.find(\"div\", attrs={\"id\":\"main\"})\n",
    "    elements = elements.findChild(\"div\", attrs={\"id\":\"main-col\"})\n",
    "    elements = elements.findChild(\"div\", attrs={\"id\":\"SearchResults\"})\n",
    "    elements = elements.findChildren(\"div\", attrs={\"id\":re.compile('searchResultImpression\\d+')}) if elements.findChildren(\"div\", attrs={\"id\":re.compile('searchResultImpression\\d+')}) else []\n",
    "\n",
    "    if 0 < len(elements):\n",
    "        i = df.shape[0]\n",
    "        for e in elements:\n",
    "            title = e.find(\"a\", attrs={\"class\":\"h4\"})\n",
    "            country = e.find(\"a\", attrs={\"class\":\"instLink\"})\n",
    "            university = e.find(\"span\", attrs={\"class\":\"phd-result__dept-inst--title\"})\n",
    "            deadline = e.find(\"div\", attrs={\"class\":\"phd-icon-area\"})\n",
    "            link = e.find(\"a\", attrs={\"class\":\"btn\"})\n",
    "            lastUpdate = e.find(\"div\", attrs={\"class\":\"apply\"})\n",
    "\n",
    "            # Handling the values that were not found\n",
    "            title = title.text if title != None else \"NaN\"\n",
    "            country = country.find(\"img\")[\"title\"] if country != None else \"NaN\"\n",
    "            university = university.text if university != None else \"NaN\"\n",
    "            if deadline != None:\n",
    "                if len(deadline.findChildren(\"a\", attrs={\"class\":\"hoverTitle\"})) != 0:\n",
    "                    deadline = deadline.findChildren(\"a\", attrs={\"class\":\"hoverTitle\"})[0].text\n",
    "                else:\n",
    "                    deadline = \"NaN\"\n",
    "            else:\n",
    "                deadline = \"NaN\"\n",
    "            if e.find(\"a\", attrs={\"class\":\"btn\"}) != None:\n",
    "                link = \"http://www.findaphd.com\"+link[\"href\"]\n",
    "            else:\n",
    "                link = \"NaN\"\n",
    "            lastUpdate = lastUpdate.text.replace(\"Updated: \",\"\").replace(\" \",\"-\").replace(\"\\n\",\"\").replace(\"\\xa0\",\"\") if lastUpdate != None else \"NaN\"\n",
    "\n",
    "            df.loc[i] = [title, country, university, deadline, lastUpdate, link]\n",
    "            i+=1\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    time.sleep(1)\n",
    "    pageNo += 1\n",
    "\n",
    "s_ = datetime.datetime.now().date().strftime(\"%d-%b-%Y\")\n",
    "df.to_excel(f\"find_a_phd-{s_}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting results for page 1\n",
      "getting results for page 2\n",
      "getting results for page 3\n",
      "getting results for page 4\n",
      "getting results for page 5\n",
      "getting results for page 6\n",
      "getting results for page 7\n",
      "getting results for page 8\n",
      "getting results for page 9\n",
      "getting results for page 10\n",
      "getting results for page 11\n",
      "getting results for page 12\n"
     ]
    }
   ],
   "source": [
    "# Crawler for www.academictransfer.com\n",
    "\n",
    "df = pd.DataFrame(columns=[\"title\",\"country\",\"university\",\"deadline\",\"posted\",\"link\"])\n",
    "\n",
    "pageNo = 1\n",
    "while True:\n",
    "    print(f\"getting results for page {pageNo}\")\n",
    "\n",
    "    response = requests.get(f\"https://www.academictransfer.com/en/jobs/?vacancy_type=scientific&q=&function_types=1&scientific_fields=3&order=&page={pageNo}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    elements = soup.findChild(\"div\", attrs={\"class\":re.compile('_search_results_\\w+')})\n",
    "    \n",
    "    if elements != None:\n",
    "        elements = elements.findChildren(\"div\", attrs={\"id\":re.compile('v\\d+')})\n",
    "        if 0 < len(elements):\n",
    "            i = df.shape[0]\n",
    "            for e in elements:\n",
    "                link = e.findChild(\"a\")\n",
    "                if link != None:\n",
    "                    link = \"https://academictransfer.com/\"+link[\"href\"]\n",
    "                else:\n",
    "                    link = \"NaN\"\n",
    "                \n",
    "                country = \"-\"\n",
    "\n",
    "                e = e.findChild(\"div\",attrs={\"class\":re.compile(\"_VacancyInfo_VacancyInfo_\\w+\")})\n",
    "                title = e.findChild(\"h6\")\n",
    "                if title != None: \n",
    "                    title = title.text.strip()\n",
    "                else:\n",
    "                    title = \"NaN\"\n",
    "                \n",
    "                e = e.findChild(\"div\",attrs={\"class\":re.compile(\"_Metadata_Metadata_-\\w+\")})\n",
    "                university = e.findChild(\"div\",attrs={\"class\":re.compile(\"_Metadata_favorite_wrapper_\\w+\")})\n",
    "                if university != None: \n",
    "                    university = university.text.strip()\n",
    "                else:\n",
    "                    university = \"NaN\"\n",
    "                    \n",
    "                deadline = e.findChild(\"div\",attrs={\"class\":re.compile(\"_Metadata_deadline_\\w+\")})\n",
    "                if deadline != None: \n",
    "                    if \"today\" in deadline.text.strip().lower():\n",
    "                        deadline = datetime.datetime.today().strftime(\"%d-%b-%Y\")\n",
    "                    elif \"yesterday\" in deadline.text.strip().lower():\n",
    "                        deadline = (datetime.datetime.today()-timedelta(1)).strftime(\"%d-%b-%Y\")\n",
    "                    else:\n",
    "                        deadline = deadline.text.strip() + \" 2024\"\n",
    "                        deadline = datetime.datetime.strptime(deadline, \"%d %b %Y\").strftime(\"%d-%b-%Y\")\n",
    "                else:\n",
    "                    deadline = \"NaN\"\n",
    " \n",
    "                posted = e.findChild(\"div\",attrs={\"class\":re.compile(\"_Metadata_created_\\w+\")})\n",
    "                if posted != None: \n",
    "                    if \"today\" in posted.text.strip().lower():\n",
    "                        posted = datetime.datetime.today().strftime(\"%d-%b-%Y\")\n",
    "                    elif \"yesterday\" in posted.text.strip().lower():\n",
    "                        posted = (datetime.datetime.today()-timedelta(1)).strftime(\"%d-%b-%Y\")\n",
    "                    else:\n",
    "                        posted = posted.text.strip() + \" 2024\"\n",
    "                        posted = datetime.datetime.strptime(posted, \"%d %b %Y\").strftime(\"%d-%b-%Y\")\n",
    "                else:\n",
    "                    posted = \"NaN\"\n",
    "\n",
    "                df.loc[i] = [title, country, university, deadline, posted, link]\n",
    "                i+=1\n",
    "            pageNo+= 1\n",
    "            time.sleep(.5)\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "_s = datetime.datetime.now().date().strftime(\"%d-%b-%Y\")\n",
    "df.to_excel(f\"academic_positions-{_s}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting results for page 1\n",
      "getting results for page 2\n",
      "getting results for page 3\n",
      "getting results for page 4\n",
      "getting results for page 5\n",
      "getting results for page 6\n",
      "getting results for page 7\n",
      "getting results for page 8\n",
      "getting results for page 9\n",
      "getting results for page 10\n"
     ]
    }
   ],
   "source": [
    "# Crawler for www.academicpositions.com\n",
    "\n",
    "df = pd.DataFrame(columns=[\"title\",\"country\",\"university\",\"deadline\",\"lastUpdate\",\"link\"])\n",
    "\n",
    "pageNo = 1\n",
    "while True:\n",
    "    print(f\"getting results for page {pageNo}\")\n",
    "\n",
    "    response = requests.get(f\"https://academicpositions.com/jobs/position/phd/field/engineering?sort=closing&page={pageNo}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    elements = soup.find(\"div\", attrs={\"id\":\"list-jobs\"})\n",
    "    \n",
    "    if elements != None:\n",
    "        elements = elements.findChildren(\"div\", attrs={\"class\":\"job-list-item\"})\n",
    "\n",
    "        if 0 < len(elements):\n",
    "            i = df.shape[0]\n",
    "            for e in elements:\n",
    "\n",
    "                title = e.findChild(\"h4\")\n",
    "                if title != None: \n",
    "                    title = title.text.strip()\n",
    "                else:\n",
    "                    title = \"NaN\"\n",
    "                    \n",
    "                country = e.findChild(\"div\", attrs={\"class\":\"job-locations\"})\n",
    "                if country != None: \n",
    "                    country = country.text.split(\",\")[-1].strip()\n",
    "                else:\n",
    "                    country = \"NaN\"\n",
    "\n",
    "                university = e.findChild(\"a\", attrs={\"class\":\"job-link\"})\n",
    "                if university != None: \n",
    "                    university = university.text.strip()\n",
    "                else:\n",
    "                    university = \"NaN\"\n",
    "                    \n",
    "                foundDeadLine = False\n",
    "                deadline = e.findChildren(\"div\", attrs={\"class\":\"text-muted\"})\n",
    "                if deadline != None:\n",
    "                    foundDeadLine = False\n",
    "                    for item in deadline:\n",
    "                        if \"Closing in:\" in item.text:\n",
    "                            deadline = item.findChildren(\"div\", attrs={\"class\":\"col-auto\"})\n",
    "                            for item in deadline:\n",
    "                                if \"Closing in:\" in item.text:\n",
    "                                    deadline = item.text.strip().replace(\"Closing in:\",\"\").strip()\n",
    "\n",
    "                                    if \"hours\" in deadline:\n",
    "                                        deadline = (datetime.datetime.now().date() + timedelta(1)).strftime(\"%d-%b-%Y\")\n",
    "                                        foundDeadLine = True\n",
    "                                    elif \"days\" in deadline:\n",
    "                                        _t = int(deadline.replace(\" days\",\"\"))\n",
    "                                        deadline = (datetime.datetime.now().date() + timedelta(_t)).strftime(\"%d-%b-%Y\")\n",
    "                                        foundDeadLine = True\n",
    "                                    elif \"-\" in deadline:\n",
    "                                        _t = deadline.replace(\" days\",\"\")\n",
    "                                        deadline = datetime.datetime.strptime(_t, \"%Y-%m-%d\").strftime(\"%d-%b-%Y\")\n",
    "                                        foundDeadLine = True\n",
    "                                    else:\n",
    "                                        deadline = \"NaN\"\n",
    "                                    # print(deadline)\n",
    "                                    foundDeadLine = True\n",
    "\n",
    "                                if not foundDeadLine:\n",
    "                                    deadline = \"NaN\"\n",
    "                                    \n",
    "                            if not foundDeadLine:\n",
    "                                deadline = \"NaN\"\n",
    "                        if not foundDeadLine:\n",
    "                            deadline = \"NaN\"\n",
    "                else:\n",
    "                    deadline = \"NaN\"\n",
    "                \n",
    "                link = e.findChildren(\"a\", attrs={\"class\":\"job-link\"})\n",
    "                if 1< len(link):\n",
    "                    link = \"https://academicpositions.com/\"+link[1][\"href\"]\n",
    "                else:\n",
    "                    link = \"NaN\"\n",
    "                    \n",
    "                df.loc[i] = [title, country, university, deadline, \"NaN\", link]\n",
    "\n",
    "                i+=1\n",
    "            pageNo+= 1\n",
    "            time.sleep(.5)\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "s_ = datetime.datetime.now().date().strftime(\"%d-%b-%Y\")\n",
    "df.to_excel(f\"academic_positions-{s_}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting results for page 1\n",
      "getting results for page 2\n",
      "getting results for page 3\n",
      "getting results for page 4\n",
      "getting results for page 5\n",
      "getting results for page 6\n",
      "getting results for page 7\n",
      "getting results for page 8\n",
      "getting results for page 9\n",
      "getting results for page 10\n",
      "getting results for page 11\n",
      "getting results for page 12\n",
      "getting results for page 13\n",
      "getting results for page 14\n",
      "getting results for page 15\n",
      "getting results for page 16\n",
      "getting results for page 17\n",
      "getting results for page 18\n",
      "getting results for page 19\n",
      "getting results for page 20\n",
      "getting results for page 21\n",
      "getting results for page 22\n",
      "getting results for page 23\n",
      "getting results for page 24\n",
      "getting results for page 25\n",
      "getting results for page 26\n",
      "getting results for page 27\n",
      "getting results for page 28\n",
      "getting results for page 29\n",
      "getting results for page 30\n",
      "getting results for page 31\n",
      "getting results for page 32\n",
      "getting results for page 33\n",
      "getting results for page 34\n",
      "getting results for page 35\n",
      "getting results for page 36\n",
      "getting results for page 37\n",
      "getting results for page 38\n",
      "getting results for page 39\n",
      "getting results for page 40\n",
      "getting results for page 41\n",
      "getting results for page 42\n",
      "getting results for page 43\n",
      "getting results for page 44\n",
      "getting results for page 45\n",
      "getting results for page 46\n",
      "getting results for page 47\n",
      "getting results for page 48\n",
      "getting results for page 49\n",
      "getting results for page 50\n",
      "getting results for page 51\n",
      "getting results for page 52\n",
      "getting results for page 53\n",
      "getting results for page 54\n",
      "getting results for page 55\n",
      "getting results for page 56\n",
      "getting results for page 57\n",
      "getting results for page 58\n",
      "getting results for page 59\n",
      "getting results for page 60\n",
      "getting results for page 61\n",
      "getting results for page 62\n",
      "getting results for page 63\n",
      "getting results for page 64\n",
      "getting results for page 65\n",
      "getting results for page 66\n",
      "getting results for page 67\n",
      "getting results for page 68\n",
      "getting results for page 69\n",
      "getting results for page 70\n",
      "getting results for page 71\n",
      "getting results for page 72\n",
      "getting results for page 73\n",
      "getting results for page 74\n",
      "getting results for page 75\n",
      "getting results for page 76\n",
      "getting results for page 77\n",
      "getting results for page 78\n",
      "getting results for page 79\n",
      "getting results for page 80\n",
      "getting results for page 81\n",
      "getting results for page 82\n",
      "getting results for page 83\n",
      "getting results for page 84\n",
      "getting results for page 85\n",
      "getting results for page 86\n",
      "getting results for page 87\n",
      "getting results for page 88\n",
      "getting results for page 89\n",
      "getting results for page 90\n",
      "getting results for page 91\n",
      "getting results for page 92\n",
      "getting results for page 93\n",
      "getting results for page 94\n",
      "getting results for page 95\n",
      "getting results for page 96\n",
      "getting results for page 97\n",
      "getting results for page 98\n",
      "getting results for page 99\n",
      "getting results for page 100\n",
      "getting results for page 101\n",
      "getting results for page 102\n",
      "getting results for page 103\n",
      "getting results for page 104\n",
      "getting results for page 105\n",
      "getting results for page 106\n",
      "getting results for page 107\n",
      "getting results for page 108\n",
      "getting results for page 109\n",
      "getting results for page 110\n",
      "getting results for page 111\n",
      "getting results for page 112\n",
      "getting results for page 113\n",
      "getting results for page 114\n",
      "getting results for page 115\n",
      "getting results for page 116\n",
      "getting results for page 117\n",
      "getting results for page 118\n",
      "getting results for page 119\n",
      "getting results for page 120\n",
      "getting results for page 121\n",
      "getting results for page 122\n",
      "getting results for page 123\n",
      "getting results for page 124\n",
      "getting results for page 125\n",
      "getting results for page 126\n",
      "getting results for page 127\n",
      "getting results for page 128\n",
      "getting results for page 129\n",
      "getting results for page 130\n",
      "getting results for page 131\n",
      "getting results for page 132\n",
      "getting results for page 133\n",
      "getting results for page 134\n",
      "getting results for page 135\n",
      "getting results for page 136\n",
      "getting results for page 137\n",
      "getting results for page 138\n",
      "getting results for page 139\n",
      "getting results for page 140\n",
      "getting results for page 141\n",
      "getting results for page 142\n",
      "getting results for page 143\n",
      "getting results for page 144\n",
      "getting results for page 145\n",
      "getting results for page 146\n",
      "getting results for page 147\n",
      "getting results for page 148\n",
      "getting results for page 149\n",
      "getting results for page 150\n",
      "getting results for page 151\n",
      "getting results for page 152\n",
      "getting results for page 153\n"
     ]
    }
   ],
   "source": [
    "# Crawler for www.euraxess.ec.europa.eu\n",
    "df = pd.DataFrame(columns=[\"title\",\"country\",\"university\",\"deadline\",\"publishDate\",\"link\"])\n",
    "\n",
    "pageNo = 1\n",
    "while True:\n",
    "    print(f\"getting results for page {pageNo}\")\n",
    "\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = Request(f\"https://euraxess.ec.europa.eu/jobs/search?f%5B0%5D=positions%3Aphp_positions&page={pageNo}\",headers=hdr)\n",
    "    page = urlopen(req)\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "    elements = (soup.findChildren(\"div\", attrs={\"id\":\"job-teaser-content\"}))\n",
    "\n",
    "    if 0 < len(elements):\n",
    "        i = df.shape[0]\n",
    "        for e in elements:\n",
    "            title = e.findChild(\"article\")\n",
    "            title = title.findChild(\"h1\")\n",
    "            if title != None:\n",
    "                title = title.text.strip()\n",
    "            else:\n",
    "                title = \"NaN\"\n",
    "            \n",
    "            country = e.findChild(\"div\", attrs={\"class\":\"label-thumbnail-wrapper\"})\n",
    "            country = country.findChildren(\"li\")\n",
    "            if 1 < len(country):\n",
    "                country = country[1].text.strip()\n",
    "            else:\n",
    "                country = \"NaN\"\n",
    "\n",
    "            university = e.findChild(\"article\")\n",
    "            university = university.findChild(\"ul\", attrs = {\"class\":\"ecl-content-block__primary-meta-container\"})\n",
    "            if university != None:\n",
    "                university = university.findChildren(\"li\")\n",
    "                if len(university) != 0:\n",
    "                    university = university[0].text\n",
    "                else:\n",
    "                    university = \"NaN\"\n",
    "            else:\n",
    "                university = \"NaN\"\n",
    "\n",
    "            publishDate = e.findChild(\"article\")\n",
    "            publishDate = publishDate.findChild(\"ul\", attrs = {\"class\":\"ecl-content-block__primary-meta-container\"})\n",
    "            if publishDate != None:\n",
    "                publishDate = publishDate.findChildren(\"li\")\n",
    "                if len(publishDate) != 0:\n",
    "                    publishDate = publishDate[1].text.replace(\"Posted on:\",\"\").strip()\n",
    "                    publishDate = datetime.datetime.strptime(publishDate, \"%d %B %Y\").strftime(\"%d-%b-%Y\")\n",
    "                else:\n",
    "                    publishDate = \"NaN\"\n",
    "            else:\n",
    "                publishDate = \"NaN\"\n",
    "                \n",
    "            deadline = e.findChild(\"article\").findChild(\"div\", attrs={\"class\",\"id-Application-Deadline\"})\n",
    "            if deadline != None:\n",
    "                deadline = deadline.text.replace(\"Application Deadline:\",\"\").strip().split(\"-\")[0].strip()\n",
    "                deadline = datetime.datetime.strptime(deadline,\"%d %b %Y\").strftime(\"%d-%b-%Y\")\n",
    "            else:\n",
    "                deadline = \"NaN\"\n",
    "            \n",
    "            link = e.findChild(\"article\")\n",
    "            link = link.findChild(\"h1\")\n",
    "            if link != None:\n",
    "                link = link.findChild(\"a\")\n",
    "                if link != None:\n",
    "                    link = \"https://euraxess.ec.europa.eu\"+link[\"href\"]\n",
    "                else:\n",
    "                    link = \"NaN\"\n",
    "            else:\n",
    "                link = \"NaN\"\n",
    "            \n",
    "            df.loc[i] = [title, country, university, deadline, publishDate, link]\n",
    "\n",
    "            i+=1\n",
    "    \n",
    "        time.sleep(0.1)\n",
    "        pageNo += 1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "s_ = datetime.datetime.now().date().strftime(\"%d-%b-%Y\")\n",
    "df.to_excel(f\"euraxess-{s_}.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".MotherVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
